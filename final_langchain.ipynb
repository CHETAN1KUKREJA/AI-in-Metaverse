{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Required Libraries\n",
    "Install the necessary libraries using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (0.3.11)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain) (3.11.10)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.24 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain) (0.3.25)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain) (0.2.3)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain) (2.10.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: sniffio in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain) (3.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from accelerate) (0.26.5)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n",
      "Requirement already satisfied: requests in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (0.45.0)\n",
      "Requirement already satisfied: torch in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from bitsandbytes) (2.5.1)\n",
      "Requirement already satisfied: numpy in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: typing_extensions>=4.8.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: networkx in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "# Install the necessary libraries using pip\n",
    "!pip install langchain\n",
    "!pip install accelerate\n",
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login to Hugging Face Hub\n",
    "Log in to Hugging Face Hub using the provided token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in to Hugging Face Hub!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace 'your_token_here' with your Hugging Face access token\n",
    "token_sukhvansh = \"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\"\n",
    "\n",
    "# Log in to Hugging Face Hub\n",
    "login(token_sukhvansh)\n",
    "\n",
    "print(\"Successfully logged in to Hugging Face Hub!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, including langchain, transformers, and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "import warnings\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers.models.mistral.modeling_mistral import MistralForCausalLM\n",
    "from transformers.models.llama.tokenization_llama_fast import LlamaTokenizerFast\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from typing import Optional, List, Mapping, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Tools\n",
    "Define the tools that will be used in the environment, such as goto, talk, trade, eat, and collect_apples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def goto(location: str, speed: int) -> str:\n",
    "    \"\"\"Go to a specific location in the environment.\n",
    "    \n",
    "    Args:\n",
    "        location: The target location to go to. Must be a valid location in the environment.\n",
    "        speed: The speed at which you want to travel to the location.\n",
    "    \"\"\"\n",
    "    return \"Reached the location.\"\n",
    "\n",
    "@tool\n",
    "def talk(agent_name: str, message: str) -> str:\n",
    "    \"\"\"Talk to another agent.\n",
    "    \n",
    "    Args:\n",
    "        agent_name: Name of the agent you want to communicate to.\n",
    "        message: The message you want to communicate to the other agent.\n",
    "    \"\"\"\n",
    "    return \"Message communicated.\"\n",
    "\n",
    "@tool\n",
    "def trade(agent_name: str, amount_of_money: int, amount_of_apples: int) -> int:\n",
    "    \"\"\"Trade with another agent.\n",
    "    \n",
    "    Args:\n",
    "        agent_name: The name of the agent to trade with.\n",
    "        amount_of_money: Amount of money you are willing to take.\n",
    "        amount_of_apples: Number of apples you are willing to give.\n",
    "    \"\"\"\n",
    "    return 50\n",
    "\n",
    "@tool\n",
    "def eat(number_of_apples: int) -> None:\n",
    "    \"\"\"Eat a specified number of apples.\n",
    "    \n",
    "    Args:\n",
    "        number_of_apples: Number of apples to eat.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "@tool\n",
    "def collect_apples(number_of_apples: int) -> None:\n",
    "    \"\"\"Collect a specified number of apples.\n",
    "    \n",
    "    Args:\n",
    "        number_of_apples: Number of apples to collect.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model and Tokenizer\n",
    "Load the model and tokenizer from Hugging Face using the specified model name and quantization configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load Model and Tokenizer\n",
    "\n",
    "# Define the model name and quantization configuration\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "# Load the model and tokenizer from Hugging Face\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, quantization_config=quantization_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Custom LLM Class\n",
    "Define a custom LLM class that extends the base LLM class and implements the required methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLLMMistral(LLM):\n",
    "    model: MistralForCausalLM\n",
    "    tokenizer: LlamaTokenizerFast\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None,\n",
    "              run_manager: Optional[CallbackManagerForLLMRun] = None) -> str:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "\n",
    "        encodeds = self.tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "        model_inputs = encodeds.to(self.model.device)\n",
    "\n",
    "        generated_ids = self.model.generate(model_inputs, max_new_tokens=512, do_sample=True, pad_token_id=tokenizer.eos_token_id, top_k=4, temperature=0.7)\n",
    "        decoded = self.tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "        output = decoded[0].split(\"[/INST]\")[1].replace(\"</s>\", \"\").strip()\n",
    "\n",
    "        if stop is not None:\n",
    "            for word in stop:\n",
    "                output = output.split(word)[0].strip()\n",
    "\n",
    "        while not output.endswith(\"```\"):\n",
    "            output += \"`\"\n",
    "\n",
    "        return output\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"model\": self.model}\n",
    "\n",
    "llm = CustomLLMMistral(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Prompt Templates\n",
    "Create the system and human prompt templates using ChatPromptTemplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "system=\"\"\"\n",
    "You are designed to solve tasks. Each task requires multiple steps that are represented by a markdown code snippet of a json blob.\n",
    "The json structure should contain the following keys:\n",
    "thought -> your thoughts\n",
    "action -> name of a tool\n",
    "action_input -> parameters to send to the tool\n",
    "\n",
    "These are the tools you can use: {tool_names}.\n",
    "\n",
    "These are the tools descriptions:\n",
    "\n",
    "{tools}\n",
    "\n",
    "If you have enough information to answer the query use the tool \"Final Answer\". Its parameters is the solution.\n",
    "If there is not enough information, keep trying.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "human=\"\"\"\n",
    "Add the word \"STOP\" after each markdown snippet. Example:\n",
    "\n",
    "```json\n",
    "{{\"thought\": \"<your thoughts>\",\n",
    " \"action\": \"<tool name or Final Answer to give a final answer>\",\n",
    " \"action_input\": \"<tool parameters or the final output\"}}\n",
    "```\n",
    "STOP\n",
    "\n",
    "This is my query=\"{input}\". Write only the next step needed to solve it.\n",
    "Your answer should be based in the previous tools executions, even if you think you know the answer.\n",
    "Remember to add STOP after each snippet.\n",
    "\n",
    "These were the previous steps given to solve this query and the information you already gathered:\n",
    "\"\"\"\n",
    "\n",
    "prompt_example = \"\"\"you are an LLM agent that is supposed to act like a human character in a virtual environment. you are given some set of actions and your job is to choose the most relevant sequence of actions in order to carry out a task in the environment. In this environment there is a forest to collect to apples with a limited supply per day and you use money to trade apples. There is a trade centre where trades can occur with other agents, and there is a house where agents can sleep.\n",
    "\n",
    "your character description: your name is Bob, you have money 50 euros and 20 apples.\n",
    "\n",
    "You are given the environment information as follows: Your location is forest in the metaverse and there is Maria agent in the forest.\n",
    "\n",
    "Current local memory your agent has: [{'action': 'goto', 'action_input': 'forest'}, {'action': 'talk',\n",
    "  'action_input': {'agent_name': 'Maria',\n",
    "   'message': 'Hello Maria, would you be interested in trading apples for money?'}},maria_reply: No I don't have any apples, {'action': 'goto', 'action_input': 'forest'}]\n",
    "\n",
    "Current actions which may have happened which concerns you: None\n",
    "\n",
    "Your goal is always to maximise the amount of money that you have and generate a valid sequence of actions you choose to do at that particular instant of time.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are an LLM agent that is supposed to act like a human character in a virtual environment. you are given some set of actions and your job is to choose the most relevant sequence of actions in order to carry out a task in the environment. In this environment there is a forest to collect to apples with a limited supply per day and you use money to trade apples. There is a trade centre where trades can occur with other agents, and there is a house where agents can sleep.\n",
      "\n",
      "your character description: your name is Bob, you have money 50 euros and 20 apples.\n",
      "\n",
      "You are given the environment information as follows: Your location is forest in the metaverse and there is Maria agent in the forest.\n",
      "\n",
      "Current local memory your agent has: [{'action': 'goto', 'action_input': 'forest'}, {'action': 'talk',\n",
      "  'action_input': {'agent_name': 'Maria',\n",
      "   'message': 'Hello Maria, would you be interested in trading apples for money?'}},maria_reply: No I don't have any apples, {'action': 'goto', 'action_input': 'forest'}]\n",
      "\n",
      "Current actions which may have happened which concerns you: None\n",
      "\n",
      "Your goal is always to maximise the amount of money that you have and generate a valid sequence of actions you choose to do at that particular instant of time.\n"
     ]
    }
   ],
   "source": [
    "from prompt.base import get_prompt\n",
    "import json\n",
    "\n",
    "with open(\"test_json.json\", \"r\") as f:\n",
    "    input_json = json.load(f)\n",
    "\n",
    "prompt = prompt_example\n",
    "# prompt = get_prompt(input_json, \"simple_chain\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Agent and Executor\n",
    "Create the agent and executor using the defined tools, LLM, and prompt templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [goto, talk, trade, eat, collect_apples]\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_system = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", human),\n",
    "        MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain.agents import create_json_chat_agent, AgentExecutor\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "agent = create_json_chat_agent(\n",
    "    tools = tools,\n",
    "    llm = llm,\n",
    "    prompt = prompt_system,\n",
    "    stop_sequence = [\"STOP\"],\n",
    "    template_tool_response = \"{observation}\"\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Agent Executor\n",
    "Run the agent executor with a sample prompt to demonstrate the functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_to_end = ['talk', 'trade', 'goto', 'collect_apples']\n",
    "tools_all = ['talk', 'trade', 'goto', 'eat', 'collect_apples']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tools_all are all the tools that the agent can use to interact with the environment and the tools_to_end are all the tools on which the chain terminates as after that interaction with the game engine is required with their feedback. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "Json_output_calls = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Json_output_calls are the final set of actions for that particular run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new None chain...\u001b[0m\n",
      "{'actions': [AgentAction(tool='talk', tool_input={'agent_name': 'Maria', 'message': \"Hello Maria, I have apples and I'd be willing to trade some for money. Would you consider making a trade?\"}, log='```json\\n{\"thought\": \"I currently have 50 euros and 20 apples. I am in the forest and Maria, who has no apples, is nearby. I can try to collect more apples, go to the trade centre to trade with other agents, or continue talking to Maria to check if her offer changes. I should maximize my money and apples. I will try to trade apples for money with Maria again.\",\\n \"action\": \"talk\",\\n \"action_input\": {\\n   \"agent_name\": \"Maria\",\\n   \"message\": \"Hello Maria, I have apples and I\\'d be willing to trade some for money. Would you consider making a trade?\"\\n }\\n}```')], 'messages': [AIMessage(content='```json\\n{\"thought\": \"I currently have 50 euros and 20 apples. I am in the forest and Maria, who has no apples, is nearby. I can try to collect more apples, go to the trade centre to trade with other agents, or continue talking to Maria to check if her offer changes. I should maximize my money and apples. I will try to trade apples for money with Maria again.\",\\n \"action\": \"talk\",\\n \"action_input\": {\\n   \"agent_name\": \"Maria\",\\n   \"message\": \"Hello Maria, I have apples and I\\'d be willing to trade some for money. Would you consider making a trade?\"\\n }\\n}```', additional_kwargs={}, response_metadata={})]}\n"
     ]
    }
   ],
   "source": [
    "for step in agent_executor.stream({\"input\": prompt}):\n",
    "    print(step)\n",
    "    if step['actions'][0].tool not in tools_all:\n",
    "        print(\"hallucination\")\n",
    "        break\n",
    "    curr_step = {}\n",
    "    curr_step['action'] = step['actions'][0].tool\n",
    "    curr_step['action_input'] = step['actions'][0].tool_input\n",
    "    Json_output_calls.append(curr_step)\n",
    "    if step['actions'][0].tool in tools_to_end:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'action': 'talk',\n",
       "  'action_input': {'agent_name': 'Maria',\n",
       "   'message': \"Hello Maria, I have apples and I'd be willing to trade some for money. Would you consider making a trade?\"}}]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Json_output_calls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing_llmbackend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
