{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json\n",
    "from chromadb import Collection\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URI_RATING=\"http://127.0.0.1:5000/rate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM('ollama_chat/llama3.1:8b', api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentMemory:\n",
    "    def __init__(self, agent_name, client_path = \"./database/mem_test\"):\n",
    "        self.agent_name = agent_name\n",
    "        self.client = chromadb.PersistentClient(path = client_path+f\"/{agent_name}\")\n",
    "        self.static_memory = self.client.get_or_create_collection(\"static_memory\")\n",
    "        self.short_term_memory = self.client.get_or_create_collection(\"short_term_memory\")\n",
    "        self.long_term_memory = self.client.get_or_create_collection(\"long_term_memory\")\n",
    "        return\n",
    "    \n",
    "    def add_static_memory(self, overwrite_existing=False, personality_path='./agent_personalities'):\n",
    "        \"\"\"\n",
    "        Adds static memory for an agent from a JSON file to the static memory database.\n",
    "        \n",
    "        Args:\n",
    "            overwrite_existing (bool): Whether to overwrite existing entries with the same IDs.\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(personality_path+f'/agent_{self.agent_name}.json') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            # Retrieve existing IDs in the database\n",
    "            existing_ids = set(self.static_memory.get()[\"ids\"])\n",
    "            \n",
    "            new_documents = []\n",
    "            new_metadatas = []\n",
    "            new_ids = []\n",
    "            \n",
    "            for doc, meta, id_ in zip(data['documents'], data['metadatas'], data['ids']):\n",
    "                if id_ in existing_ids:\n",
    "                    if overwrite_existing:\n",
    "                        # Overwrite: Remove existing entry first\n",
    "                        self.static_memory.delete(ids=[id_])\n",
    "                        new_documents.append(doc)\n",
    "                        new_metadatas.append(meta)\n",
    "                        new_ids.append(id_)\n",
    "                    else:\n",
    "                        # Skip: Do not add duplicate entries\n",
    "                        print(f\"Skipped adding memory with ID: {id_} (already exists)\")\n",
    "                else:\n",
    "                    # Add new entries\n",
    "                    new_documents.append(doc)\n",
    "                    new_metadatas.append(meta)\n",
    "                    new_ids.append(id_)\n",
    "            \n",
    "            # Add new/updated data to the database\n",
    "            if new_ids:\n",
    "                self.static_memory.add(\n",
    "                    documents=new_documents,\n",
    "                    metadatas=new_metadatas,\n",
    "                    ids=new_ids,\n",
    "                )\n",
    "                print(f\"Added {len(new_ids)} static memories for agent {self.agent_name} successfully\")\n",
    "            else:\n",
    "                print(f\"No new static memories added for agent {self.agent_name}\")\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def add_short_term_memory(self, event, timestamp=None, overwrite_existing=False):\n",
    "        \"\"\"\n",
    "        Adds a log of an event to the short-term memory.\n",
    "        \n",
    "        Args:\n",
    "            event (str): The event to be logged.\n",
    "            timestamp (str): The timestamp of the event. Defaults to the current time if not provided.\n",
    "            overwrite_existing (bool): Whether to overwrite existing entries with the same ID.\n",
    "        \"\"\"\n",
    "        if timestamp is None:\n",
    "            timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        importance_score = self._initial_recency_and_importance_score_calculator(event,timestamp)\n",
    "        print(importance_score)\n",
    "        event_id = f\"stm_{timestamp}\"\n",
    "        \n",
    "        \"\"\" \n",
    "            for development purpose only to prevent multiple ids of same timestamp. though really difficult \n",
    "            to have such a event but for preventive mesures.If the agent want to rewrite a previous event then it would be better to have a \n",
    "            new event rather than correting a previous one\n",
    "        \"\"\"\n",
    "        \n",
    "        # Retrieve existing IDs in the database\n",
    "        existing_ids = set(self.short_term_memory.get()[\"ids\"])\n",
    "        if event_id in existing_ids:\n",
    "            if overwrite_existing:\n",
    "                # Overwrite: Remove existing entry first\n",
    "                self.short_term_memory.delete(ids=[event_id])\n",
    "                self.short_term_memory.add(\n",
    "                    documents=[event], \n",
    "                    metadatas=[{\"timestamp\": timestamp, \"importance_score\": importance_score}], \n",
    "                    ids=[event_id]\n",
    "                )\n",
    "                print(f\"Overwritten event in short-term memory with ID: {event_id}\")\n",
    "            else:\n",
    "                # Skip: Do not add duplicate entry\n",
    "                print(f\"Skipped adding event to short-term memory (ID already exists): {event_id}\")\n",
    "        else:\n",
    "            # Add new entry\n",
    "            self.short_term_memory.add(\n",
    "                    documents=[event],\n",
    "                    metadatas=[{\"timestamp\": timestamp, \"importance_score\": importance_score}], \n",
    "                    ids=[event_id]\n",
    "                )\n",
    "            print(f\"Added event to short-term memory: {event}\")\n",
    "        return\n",
    "    \n",
    "    def migrate_to_long_term_memory(self):\n",
    "        \"\"\"\n",
    "        Migrates the oldest document from short-term memory to long-term memory.\n",
    "        The document is deleted from short-term memory fter migration.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if migration was successful, False if no documents to migrate\n",
    "        \"\"\"\n",
    "        # Get all documents from short-term memory\n",
    "        stm_data = self.short_term_memory.get()\n",
    "        \n",
    "        length_stm = len(stm_data['documents'])\n",
    "        \n",
    "        # Check if there are any documents to migrate\n",
    "        if length_stm <= 3 :\n",
    "            print(\"Not enough documents to migrate to long-term memory\")\n",
    "            return\n",
    "            \n",
    "        # Zip together documents, metadata, and ids for easier processing\n",
    "        memory_entries = list(zip(\n",
    "            stm_data['documents'], \n",
    "            stm_data['metadatas'], \n",
    "            stm_data['ids']\n",
    "        ))\n",
    "        \n",
    "        # Sort by timestamp in ascending order to get the oldest entry\n",
    "        oldest_entries = sorted(\n",
    "            memory_entries,\n",
    "            key=lambda x: x[1]['timestamp']\n",
    "        )[:length_stm-3]\n",
    "        \n",
    "        for i in range(len(oldest_entries)):\n",
    "        \n",
    "            oldest_document, oldest_metadata, oldest_id = oldest_entries[i]\n",
    "            \n",
    "            try:\n",
    "                # Delete from short-term memory\n",
    "                self.short_term_memory.delete(\n",
    "                    ids=[oldest_id]\n",
    "                )\n",
    "                \n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "            if(oldest_metadata['importance_score'] < 0.5):\n",
    "                continue\n",
    "            \n",
    "            # Create new ID for long-term memory\n",
    "            ltm_id = f\"ltm_{oldest_metadata['timestamp']}\"\n",
    "            \n",
    "            try:\n",
    "                # Add to long-term memory\n",
    "                self.long_term_memory.add(\n",
    "                    documents=[oldest_document],\n",
    "                    metadatas=[oldest_metadata],\n",
    "                    ids=[ltm_id]\n",
    "                )\n",
    "                \n",
    "                print(f\"Successfully migrated memory from short-term to long-term: {oldest_document}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error during migration: {str(e)}\")\n",
    "    \n",
    "    def decay_long_term_memory(self):\n",
    "        \"\"\"\n",
    "        Decays the importance of long-term memories.\n",
    "        \"\"\"\n",
    "        ltm = self.long_term_memory.get(include=['documents', 'metadatas'])\n",
    "        for doc, meta, id_ in zip(ltm['documents'], ltm['metadatas'], ltm['ids']):\n",
    "            if 'importance' in meta:\n",
    "                meta['importance'] *= 0.9\n",
    "                if meta['importance'] < 1:\n",
    "                    self.long_term_memory.delete(ids=[id_])\n",
    "                else:\n",
    "                    self.long_term_memory.update(ids=[id_], metadatas=[meta])\n",
    "        return\n",
    "        \n",
    "    def summarize_and_forget(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        summarize the 5 least important memories in the long-term memory and delete them.\n",
    "        \n",
    "        \"\"\"\n",
    "        ltm_data = self.long_term_memory.get()\n",
    "        if len(ltm_data['documents']) >= 5:\n",
    "\n",
    "            ltm_entries = list(zip(\n",
    "                ltm_data['documents'],\n",
    "                ltm_data['metadatas'],\n",
    "                ltm_data['ids']\n",
    "            ))\n",
    "            \n",
    "            \n",
    "            least_important = sorted(\n",
    "                ltm_entries,\n",
    "                key=lambda x: x[1]['importance_score']\n",
    "            )[:5]\n",
    "            \n",
    "\n",
    "            combined_content = \" \".join(entry[0] for entry in least_important)\n",
    "            avg_importance = sum(entry[1]['importance_score'] for entry in least_important) / 5\n",
    "            \n",
    "            try:\n",
    "                question = f\"Summarize the following content in exactly two lines:\\n{combined_content}\"\n",
    "                gen = dspy.Predict('question -> answer')\n",
    "                summary = gen(question = question).answer\n",
    "                \n",
    "                old_ids = [entry[2] for entry in least_important]\n",
    "                self.long_term_memory.delete(ids=old_ids)\n",
    "                \n",
    "                # Add consolidated summary\n",
    "                new_metadata = {\n",
    "                    'timestamp': time.time(),\n",
    "                    'importance_score': avg_importance,\n",
    "                    'type': 'consolidated_summary'\n",
    "                }\n",
    "                \n",
    "                self.long_term_memory.add(\n",
    "                    documents=[summary],\n",
    "                    metadatas=[new_metadata],\n",
    "                    ids=[f\"consolidated_{int(time.time())}\"]\n",
    "                )\n",
    "                \n",
    "                print(f\"Successfully consolidated {len(least_important)} memories into summary\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error during consolidation: {str(e)}\")\n",
    "                \n",
    "        else:\n",
    "            print(\"Not enough memories in long-term memory to consolidate\")\n",
    "    \n",
    "    def query_memory(self, prompt: str, top_k: int = 3, score_threshold: float = 0.35) -> list[str]:\n",
    "        \"\"\"\n",
    "        Smart querying mechanism to retrieve context from memories.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The query or prompt for which context is being retrieved.\n",
    "            top_k (int): Number of top matches to retrieve from each memory.\n",
    "            score_threshold (float): Threshold to filter out low score context.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: The final context selected after smart querying and validation.\n",
    "        \"\"\"\n",
    "        # Helper function to query a memory based on total score, not just similarity\n",
    "        def query_local_memory(\n",
    "                memory:Collection, \n",
    "                prompt:str, \n",
    "                top_k:int, \n",
    "                similarity_weight:float=1.0, \n",
    "                importance_weight:float=1.0\n",
    "            ) -> list[str]:\n",
    "            # Must change this to query based on importance score instead\n",
    "            # First, query everything\n",
    "            # query() return list of list of objects, while get() only returns list of objects\n",
    "            try :\n",
    "                results = memory.query(\n",
    "                    query_texts=[prompt],\n",
    "                    n_results=memory.count(),\n",
    "                    include=[\"metadatas\", \"distances\"]\n",
    "                )\n",
    "            except:\n",
    "                return []\n",
    "            # Now, calculate the similarity and total score\n",
    "            similarities = [1/(1+dist) for dist in results[\"distances\"][0]]\n",
    "            # If the similarities are None, that means the query returns no results -> return empty context\n",
    "            if not similarities:\n",
    "                return []\n",
    "            scores = np.array([\n",
    "                (\n",
    "                    importance_weight * meta['importance_score'] + \n",
    "                    similarity_weight * sim\n",
    "                ) if 'importance_score' in meta else similarity_weight * sim\n",
    "                for meta, sim in zip(results[\"metadatas\"][0], similarities)\n",
    "            ])\n",
    "            # Sort the scores in descending order, get the top k ids\n",
    "            top_k_indices = np.argsort(scores)[::-1][:top_k].astype(int)\n",
    "            top_k_ids = [results[\"ids\"][0][i] for i in top_k_indices]\n",
    "            print(f\"top_k_ids: {top_k_ids}\")\n",
    "            # Get the documents, metadatas, distances\n",
    "            final_results = memory.get(\n",
    "                ids=top_k_ids,\n",
    "                include=[\"documents\", \"metadatas\"]\n",
    "            )\n",
    "            documents = final_results[\"documents\"] # list of string already\n",
    "            metadatas = final_results[\"metadatas\"] # list of json strings\n",
    "            final_scores = scores[top_k_indices]\n",
    "            distances = np.array(results[\"distances\"][0])[top_k_indices]\n",
    "            print(f\"final_scores: {final_scores}\")\n",
    "            \n",
    "            context_local = []\n",
    "            for doc, meta, dist, score in zip(documents, metadatas, distances, final_scores):\n",
    "                # Filter out irrelevant context\n",
    "                if score >= score_threshold:\n",
    "                    context_local.append(\n",
    "                        {\n",
    "                            \"document\": doc, \n",
    "                            \"metadata\": meta, \n",
    "                            \"distances\": dist,\n",
    "                            \"score\": score\n",
    "                        }\n",
    "                    )\n",
    "            return context_local\n",
    "\n",
    "        # Priority order: Short-Term & Static -> Long-Term\n",
    "        context = []\n",
    "        for memory_name, memory in [(\"short_term\", self.short_term_memory), (\"static\", self.static_memory)]:\n",
    "            results = query_local_memory(memory, prompt, top_k)\n",
    "            # Maybe change list to set\n",
    "            context.extend(results)\n",
    "            if len(context) > top_k:\n",
    "                break  # Stop if we have enough context\n",
    "\n",
    "        # If context is still insufficient, fall back to long-term memory\n",
    "        if len(context) < top_k:\n",
    "            print(\"Falling back to long-term memory...\")\n",
    "            long_term_results = query_local_memory(self.long_term_memory, prompt, top_k - len(context))\n",
    "            context.extend(long_term_results)\n",
    "\n",
    "        # Sort the context by highest score in descending order\n",
    "        if context:\n",
    "            context = sorted(context, key=lambda x: x[\"score\"], reverse=True)\n",
    "        return self._format_context_output(prompt, context)\n",
    "    \n",
    "    def plan_from_memory(\n",
    "            self, \n",
    "            timestamp: str = None,\n",
    "            top_k: int = 20, \n",
    "            score_threshold: float = 0.35\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        (NOT DONE! DON'T USE YET) Plan an action based on the context from memories and add it to long-term memory.\n",
    "\n",
    "        Args:\n",
    "            timestamp (str): The timestamp of the current game time. Defaults to the current time if not provided.\n",
    "            top_k (int): Number of top matches to retrieve from all memory.\n",
    "            score_threshold (float): Threshold to filter out low score context.\n",
    "        \"\"\"\n",
    "\n",
    "        prompt_for_context = \"\"\"\n",
    "            Answer the following question based on the context from memories:\n",
    "            What is your name?\n",
    "            What is the summary of your character and traits?\n",
    "            What is your passion project currently, and how do you plan to achieve it?\n",
    "            What actions did you do recently, up until 1 day ago?\n",
    "        \"\"\"\n",
    "\n",
    "        context = self.query_memory(\n",
    "            prompt_for_context, \n",
    "            top_k=top_k, \n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "\n",
    "        if timestamp is None:\n",
    "            timestamp = datetime.now().isoformat()\n",
    "\n",
    "        prompt_for_plan = f\"\"\"\n",
    "            Based on the given context, fill in the following template. \n",
    "            Do not hallucinate. If you don't know the answer, say \"I don't know\".\n",
    "            <plan> is in format: 1) <action_1> at <time_1> 2) <action_2> at <time_2> 3) <action_3> at <time_3> ...\n",
    "            Context: {context}\n",
    "\n",
    "            Only return this part:\n",
    "\n",
    "            Name: <name>\n",
    "            Traits: <traits>\n",
    "            Passion Project: <passion_project>\n",
    "            Recent Actions: <recent_actions>\n",
    "            Today is {timestamp}. Here is <name>'s plan today in broad strokes:\n",
    "            <plan>\n",
    "        \"\"\"\n",
    "\n",
    "        ## get the result from LLM server\n",
    "        try:\n",
    "            # response = requests.post(<URI>,json={'memory': prompt_for_plan})\n",
    "            # response.raise_for_status()  # Raise an exception for bad status codes\n",
    "            # result = response.json()\n",
    "            # if isinstance(result, str):\n",
    "                # result = json.loads(result) \n",
    "\n",
    "            # TODO: change this format to be compatible with the model's return\n",
    "            # plan = result['plan']\n",
    "            plan = \"Plan: \" + plan\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error calling server: {e}\")\n",
    "\n",
    "        ## add to long-term memory, overwrite the old plan\n",
    "        # delete the old plan if it exists\n",
    "        try:    \n",
    "            self.long_term_memory.delete(ids=[f\"plan_1\"])\n",
    "        except: pass\n",
    "        # add the new plan\n",
    "        self.long_term_memory.add(\n",
    "            documents=[plan],\n",
    "            metadatas=[{\"timestamp\": timestamp, \"importance_score\": 8}], # Force model to prioritize planning with high score\n",
    "            ids=[f\"plan_1\"] # There can be only one plan per agent per day.\n",
    "        )\n",
    "\n",
    "    def get_stats(self):\n",
    "        \"\"\"\n",
    "        Get statistics of the memory.\n",
    "        \n",
    "        Args:\n",
    "            memory_type (str): The type of memory to get statistics for. Can be 'static', 'short-term', 'long-term', or 'all'.\n",
    "        \n",
    "        Returns:\n",
    "            dict: The statistics of the memory.\n",
    "        \"\"\"\n",
    "        static_stats = self._get_stats(self.static_memory)\n",
    "        short_term_stats = self._get_stats(self.short_term_memory)\n",
    "        long_term_stats = self._get_stats(self.long_term_memory)\n",
    "        return {\n",
    "            \"static_memory\": static_stats,\n",
    "            \"short_term_memory\": short_term_stats,\n",
    "            \"long_term_memory\": long_term_stats,\n",
    "        }\n",
    "\n",
    "    def calculate_recency_score(timestamp: str, current_time: str = None, decay_factor: float = 0.995) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the recency score using an exponential decay function.\n",
    "        \n",
    "        Args:\n",
    "            timestamp (str): The timestamp string in ISO format of when the memory was created/last accessed\n",
    "            current_time (str): The current time string in ISO format to compare against (default: current time)\n",
    "            decay_factor (float): The decay factor for the exponential decay (default: 0.995)\n",
    "        \n",
    "        Returns:\n",
    "            float: A recency score between 0 and 1, where 1 indicates most recent\n",
    "        \"\"\"\n",
    "        if timestamp is None:\n",
    "            return 1.0  # Return maximum score if no timestamp provided\n",
    "            \n",
    "        # Convert timestamp strings to datetime objects\n",
    "        if current_time is None:\n",
    "            current_time = datetime.now().isoformat()\n",
    "            \n",
    "        timestamp_dt = datetime.fromisoformat(timestamp)\n",
    "        current_time_dt = datetime.fromisoformat(current_time)\n",
    "        \n",
    "        # Calculate the time difference in hours\n",
    "        time_diff = current_time_dt - timestamp_dt\n",
    "        hours_diff = time_diff.total_seconds() / (3600*3)\n",
    "        \n",
    "        # Calculate recency score using exponential decay\n",
    "        recency_score = math.pow(decay_factor, hours_diff)\n",
    "        \n",
    "        # Ensure the score is between 0 and 1\n",
    "        recency_score = max(0.0, min(1.0, recency_score))\n",
    "        \n",
    "        return recency_score\n",
    "\n",
    "    def _initial_recency_and_importance_score_calculator(self,doc: str, timestamp: str):\n",
    "        \"\"\"\n",
    "        Calculate the importance score of a document. \n",
    "        The parameters considered are Relevency, Recency, Frequency, Personal Impact\n",
    "\n",
    "        Args:\n",
    "            doc (str): The document text\n",
    "            timestamp (str): ISO format timestamp string\n",
    "        \"\"\"\n",
    "        recency = self.calculate_recency_score(timestamp)\n",
    "\n",
    "        # Call the memory rating server\n",
    "        try:\n",
    "            response = requests.post(URI_RATING,json={'memory': doc})\n",
    "            response.raise_for_status()  # Raise an exception for bad status codes\n",
    "            result = response.json()\n",
    "            if isinstance(result, str):\n",
    "                result = json.loads(result) \n",
    "\n",
    "            print(result[\"rating\"])  # Now you can access the rating\n",
    "            personal_impact = int(result['rating'])    \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error calling memory rating server: {e}\")\n",
    "            personal_impact = 5  # Default value if the server call fails\n",
    "\n",
    "        weights = {\n",
    "            \"recency\": 0.25,\n",
    "            \"personal_impact\": 0.25\n",
    "        }\n",
    "        \n",
    "        importance_score = (\n",
    "            recency * weights[\"recency\"] + \n",
    "            personal_impact * weights[\"personal_impact\"]\n",
    "        )\n",
    "        \n",
    "        return importance_score\n",
    "\n",
    "    def _format_context_output(self, prompt: str, context: list[str], separator: str = \"\\n---\\n\") -> str:\n",
    "        \"\"\"\n",
    "        Formats the retrieved context into a final text output with separators.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The input prompt for which context is being retrieved.\n",
    "            context (List[str]): List of retrieved documents to be used as context.\n",
    "            separator (str): Separator string to distinguish between context documents.\n",
    "\n",
    "        Returns:\n",
    "            str: A formatted text output containing the context for the given prompt.\n",
    "        \"\"\"      \n",
    "        # Header for the final output\n",
    "        formatted_output = f\"Context for Prompt: '{prompt}'\\n\\n\"\n",
    "\n",
    "        # NOTE: handle in case no matched context found here\n",
    "        if not context:\n",
    "            formatted_output += \"No important context found\"\n",
    "            return formatted_output\n",
    "\n",
    "        # Add each document separated by the specified separator\n",
    "        for idx, document in enumerate(context, 1):\n",
    "            # formatted_output += f\"[Document {idx}]:\\n{document['document'], document['distances']}{separator}\"\n",
    "            formatted_output += f\"[Document {idx}]:\\n{document['document']}{separator}\"\n",
    "\n",
    "        # Remove the last separator for cleaner formatting\n",
    "        formatted_output = formatted_output.rstrip(separator)\n",
    "\n",
    "        return formatted_output\n",
    "    \n",
    "    def _get_stats(self, memory:Collection):\n",
    "        \"\"\"\n",
    "        Get statistics of the memory/vector database.\n",
    "\n",
    "        Args:\n",
    "            memory (Collection): The memory to get statistics for.\n",
    "        \"\"\"\n",
    "        memory_data = memory.get()\n",
    "        n_documents = len(memory_data[\"documents\"])\n",
    "        return {\n",
    "            \"n_documents\": n_documents,\n",
    "        }\n",
    "    \n",
    "    def clear_memory(self, memory_type=\"all\"):\n",
    "        \"\"\"\n",
    "        Clear all the memory of specified type.\n",
    "        \n",
    "        Args:\n",
    "            memory_type (str): The type of memory to clear. Can be 'static', 'short-term', 'long-term', or 'all'.\n",
    "        \"\"\"\n",
    "        if memory_type == \"all\":\n",
    "            try: self.static_memory.delete(ids=self.static_memory.get()[\"ids\"])\n",
    "            except: pass    # Memory is already empty\n",
    "            try: self.short_term_memory.delete(ids=self.short_term_memory.get()[\"ids\"])\n",
    "            except: pass    # Memory is already empty\n",
    "            try: self.long_term_memory.delete(ids=self.long_term_memory.get()[\"ids\"])\n",
    "            except: pass    # Memory is already empty\n",
    "            print(\"All memory cleared successfully\")\n",
    "        elif memory_type == \"static\":\n",
    "            try: self.static_memory.delete(ids=self.static_memory.get()[\"ids\"])\n",
    "            except: pass    # Memory is already empty\n",
    "            print(\"Static memory cleared successfully\")\n",
    "        elif memory_type == \"short-term\":\n",
    "            try: self.short_term_memory.delete(ids=self.short_term_memory.get()[\"ids\"])\n",
    "            except: pass    # Memory is already empty\n",
    "            print(\"Short-term memory cleared successfully\")\n",
    "        elif memory_type == \"long-term\":\n",
    "            try: self.long_term_memory.delete(ids=self.long_term_memory.get()[\"ids\"])\n",
    "            except: pass    # Memory is already empty\n",
    "            print(\"Long-term memory cleared successfully\")\n",
    "        else:\n",
    "            print(\"Invalid memory type. Please specify 'static', 'short-term', 'long-term', or 'all'.\")\n",
    "            \n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped adding memory with ID: char_1 (already exists)\n",
      "Skipped adding memory with ID: rule_1 (already exists)\n",
      "Skipped adding memory with ID: env_1 (already exists)\n",
      "Skipped adding memory with ID: rule_2 (already exists)\n",
      "No new static memories added for agent alex\n"
     ]
    }
   ],
   "source": [
    "agent_name = \"alex\"\n",
    "\n",
    "agent_memory = AgentMemory(agent_name)\n",
    "\n",
    "agent_memory.add_static_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ADD DUMMY DATA TO SHORT TERM AND LONG TERM MEMORY\n",
    "\n",
    "# with open (\"./agent_personalities/agent_alex_short_mem.json\") as f:\n",
    "#     stm_data = json.load(f)\n",
    "\n",
    "# with open (\"./agent_personalities/agent_alex_long_mem.json\") as f:\n",
    "#     ltm_data = json.load(f)\n",
    "\n",
    "# agent_memory.short_term_memory.add(documents=stm_data['documents'], metadatas=stm_data['metadatas'], ids=stm_data['ids'])\n",
    "# agent_memory.long_term_memory.add(documents=ltm_data['documents'], metadatas=ltm_data['metadatas'], ids=ltm_data['ids'])\n",
    "\n",
    "# print()\n",
    "# print(agent_memory.get_stats())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n",
      "Number of requested results 5 is greater than number of elements in index 4, updating n_results = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_k_ids: ['exp_2', 'exp_1', 'exp_3']\n",
      "final_scores: [0.4404222  0.36492197 0.35260854]\n",
      "top_k_ids: ['env_1', 'rule_1', 'char_1', 'rule_2']\n",
      "final_scores: [0.51141153 0.43352968 0.37362446 0.32506791]\n",
      "Retrieved Context:\n",
      "Context for Prompt: 'What did i do in the forest?'\n",
      "\n",
      "[Document 1]:\n",
      "Name: Alex, Age: 25, Role: Warrior\n",
      "---\n",
      "[Document 2]:\n",
      "Timestamp: 2023-10-01T10:00:00Z, Location: Coastal Town, Interaction: Met with local fishermen to discuss the impact of recent storms on fish populations, Actions: Collected water samples for analysis, Responses: Fishermen expressed concerns about declining fish stocks\n",
      "---\n",
      "[Document 3]:\n",
      "Rule: Fire burns wood\n",
      "---\n",
      "[Document 4]:\n",
      "Environment: The forest is dense with trees and has wild animals\n",
      "---\n",
      "[Document 5]:\n",
      "Timestamp: 2023-10-02T14:30:00Z, Location: Forest, Interaction: Observed wildlife and noted changes in animal behavior due to seasonal shifts, Actions: Set up camera traps to monitor animal movements, Responses: Noticed an increase in nocturnal activity among certain species\n",
      "---\n",
      "[Document 6]:\n",
      "Timestamp: 2023-10-03T09:00:00Z, Location: Research Lab, Interaction: Analyzed water samples collected from the coastal town, Actions: Conducted tests to measure pollution levels, Responses: Found elevated levels of contaminants, likely from recent industrial activity\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "prompt = \"What is my name?\"\n",
    "context = agent_memory.query_memory(prompt, top_k=5, score_threshold=0.35)\n",
    "print(\"Retrieved Context:\")\n",
    "print(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['exp_1', 'exp_2', 'exp_3'],\n",
       " 'embeddings': None,\n",
       " 'documents': ['Timestamp: 2023-10-01T10:00:00Z, Location: Coastal Town, Interaction: Met with local fishermen to discuss the impact of recent storms on fish populations, Actions: Collected water samples for analysis, Responses: Fishermen expressed concerns about declining fish stocks',\n",
       "  'Timestamp: 2023-10-02T14:30:00Z, Location: Forest, Interaction: Observed wildlife and noted changes in animal behavior due to seasonal shifts, Actions: Set up camera traps to monitor animal movements, Responses: Noticed an increase in nocturnal activity among certain species',\n",
       "  'Timestamp: 2023-10-03T09:00:00Z, Location: Research Lab, Interaction: Analyzed water samples collected from the coastal town, Actions: Conducted tests to measure pollution levels, Responses: Found elevated levels of contaminants, likely from recent industrial activity'],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [{'type': 'recent_experience'},\n",
       "  {'type': 'recent_experience'},\n",
       "  {'type': 'recent_experience'}],\n",
       " 'included': [<IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_memory.short_term_memory.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [],\n",
       " 'embeddings': None,\n",
       " 'documents': [],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [],\n",
       " 'included': [<IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_memory.long_term_memory.get()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env_achal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
