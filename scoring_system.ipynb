{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "lm = dspy.LM('ollama_chat/llama3.1:8b', api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer1 = dspy.Predict(signature=\"statement, personality, question-> impact_score, relevance_score\")\n",
    "scorer2 = dspy.ChainOfThought(signature=\"statement, personality, question-> impact_score, relevance_score\")\n",
    "# scorer3 = dspy.ProgramOfThought(signature=\"statement, personality, question-> impact_score\")\n",
    "scorer4 = dspy.ReAct(signature=\"statement, personality, question-> impact_score, relevance_score\", tools=[])\n",
    "scorer5 = dspy.MultiChainComparison(signature=\"statement, personality, question-> impact_score, relevance_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"You are given a statement and a personality. Determine the impact and relevance of the statement based on the personality on a scale from 0 to 1 with 0 meaning no relevance and 1 being highly relevant. The impact score shows how much the statement affects the personality. The relevance score shows how much the statement is relevant or close to the personality.\"\n",
    "statement = \"Bob did not help his friends with their homework.\"\n",
    "personality = \"Bob is a helpful person. He is always willing to lend a hand to his friends.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer1 = scorer1(question=question, statement=statement, personality=personality)\n",
    "answer2 = scorer2(question=question, statement=statement, personality=personality)\n",
    "# answer3 = scorer3(question=question, statement=statement, personality=personality)\n",
    "answer4 = scorer4(question=question, statement=statement, personality=personality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The number of attempts (0) doesn't match the expected number M (3). Please set the correct value for M when initializing MultiChainComparison.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m answer5 \u001b[38;5;241m=\u001b[39m \u001b[43mscorer5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersonality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersonality\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_env_achal/lib/python3.9/site-packages/dspy/utils/callback.py:234\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.wrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# If no callbacks are provided, just call the function\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# Generate call ID as the unique identifier for the call, this is useful for instrumentation.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m call_id \u001b[38;5;241m=\u001b[39m uuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_env_achal/lib/python3.9/site-packages/dspy/primitives/program.py:24\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;129m@with_callbacks\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_env_achal/lib/python3.9/site-packages/dspy/predict/multi_chain_comparison.py:45\u001b[0m, in \u001b[0;36mMultiChainComparison.forward\u001b[0;34m(self, completions, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     answer \u001b[38;5;241m=\u001b[39m c[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_key]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     41\u001b[0m     attempts\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m«I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm trying to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrationale\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm not sure but my prediction is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m»\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     43\u001b[0m     )\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(attempts) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mM, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of attempts (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(attempts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match the expected number M (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mM\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Please set the correct value for M when initializing MultiChainComparison.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_attempt_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m: attempt\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     53\u001b[0m }\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mAssertionError\u001b[0m: The number of attempts (0) doesn't match the expected number M (3). Please set the correct value for M when initializing MultiChainComparison."
     ]
    }
   ],
   "source": [
    "answer5 = scorer5(question=question, statement=statement, personality=personality, completions=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT:  Prediction(\n",
      "    impact_score='0.6',\n",
      "    relevance_score='0.4'\n",
      ")\n",
      "CHAIN OF THOUGHT:  Prediction(\n",
      "    reasoning='The statement \"Bob did not help his friends with their homework\" contradicts Bob\\'s personality of being a helpful person. This means that the impact score will be low, as it goes against what is expected of him. The relevance score will also be low because the action described in the statement does not align with his typical behavior.',\n",
      "    impact_score='0.2',\n",
      "    relevance_score='0.3'\n",
      ")\n",
      "REACT:  Prediction(\n",
      "    trajectory={'thought_0': \"The statement seems to contradict Bob's personality.\", 'tool_name_0': 'finish', 'tool_args_0': {}, 'observation_0': 'Completed.'},\n",
      "    reasoning=\"The statement contradicts Bob's personality, indicating a low impact score since it goes against his helpful nature. The relevance score is high because the statement directly relates to Bob's character.\",\n",
      "    impact_score='0.2',\n",
      "    relevance_score='0.8'\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"PREDICT: \", answer1)\n",
    "print(\"CHAIN OF THOUGHT: \", answer2)\n",
    "# print(\"PROGRAM OF THOUGHT: \", answer3)\n",
    "print(\"REACT: \", answer4)\n",
    "# print(\"MULTI CHAIN COMPARISON: \", answer5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-01-09T18:36:31.775191]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `statement` (str)\n",
      "2. `personality` (str)\n",
      "3. `question` (str)\n",
      "4. `trajectory` (str)\n",
      "\n",
      "Your output fields are:\n",
      "1. `next_thought` (str)\n",
      "2. `next_tool_name` (typing.Literal[finish])\n",
      "3. `next_tool_args` (dict[str, typing.Any])\n",
      "\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## statement ## ]]\n",
      "{statement}\n",
      "\n",
      "[[ ## personality ## ]]\n",
      "{personality}\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## trajectory ## ]]\n",
      "{trajectory}\n",
      "\n",
      "[[ ## next_thought ## ]]\n",
      "{next_thought}\n",
      "\n",
      "[[ ## next_tool_name ## ]]\n",
      "{next_tool_name}        # note: the value you produce must be one of: finish\n",
      "\n",
      "[[ ## next_tool_args ## ]]\n",
      "{next_tool_args}        # note: the value you produce must be pareseable according to the following JSON schema: {\"type\": \"object\"}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "In adhering to this structure, your objective is: \n",
      "        Given the fields `statement`, `personality`, `question`, produce the fields `impact_score`, `relevance_score`.\n",
      "        \n",
      "        You will be given `statement`, `personality`, `question` and your goal is to finish with `impact_score`, `relevance_score`.\n",
      "        \n",
      "        To do this, you will interleave Thought, Tool Name, and Tool Args, and receive a resulting Observation.\n",
      "        \n",
      "        Thought can reason about the current situation, and Tool Name can be the following types:\n",
      "        \n",
      "        (1) finish, whose description is <desc>Signals that the final outputs, i.e. `impact_score`, `relevance_score`, are now available and marks the task as complete.</desc>. It takes arguments {} in JSON format.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## statement ## ]]\n",
      "Bob did not help his friends with their homework.\n",
      "\n",
      "[[ ## personality ## ]]\n",
      "Bob is a helpful person. He is always willing to lend a hand to his friends.\n",
      "\n",
      "[[ ## question ## ]]\n",
      "You are given a statement and a personality. Determine the impact and relevance of the statement based on the personality on a scale from 0 to 1 with 0 meaning no relevance and 1 being highly relevant. The impact score shows how much the statement affects the personality. The relevance score shows how much the statement is relevant or close to the personality.\n",
      "\n",
      "[[ ## trajectory ## ]]\n",
      "\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## next_thought ## ]]`, then `[[ ## next_tool_name ## ]]` (must be formatted as a valid Python typing.Literal[finish]), then `[[ ## next_tool_args ## ]]` (must be formatted as a valid Python dict[str, typing.Any]), and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## next_thought ## ]]\n",
      "The statement seems to contradict Bob's personality.\n",
      "\n",
      "[[ ## next_tool_name ## ]]\n",
      "finish\n",
      "\n",
      "[[ ## next_tool_args ## ]]\n",
      "{}\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-01-09T18:36:42.859806]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `statement` (str)\n",
      "2. `personality` (str)\n",
      "3. `question` (str)\n",
      "4. `trajectory` (str)\n",
      "\n",
      "Your output fields are:\n",
      "1. `reasoning` (str)\n",
      "2. `impact_score` (str)\n",
      "3. `relevance_score` (str)\n",
      "\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## statement ## ]]\n",
      "{statement}\n",
      "\n",
      "[[ ## personality ## ]]\n",
      "{personality}\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## trajectory ## ]]\n",
      "{trajectory}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## impact_score ## ]]\n",
      "{impact_score}\n",
      "\n",
      "[[ ## relevance_score ## ]]\n",
      "{relevance_score}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "In adhering to this structure, your objective is: \n",
      "        Given the fields `statement`, `personality`, `question`, produce the fields `impact_score`, `relevance_score`.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## statement ## ]]\n",
      "Bob did not help his friends with their homework.\n",
      "\n",
      "[[ ## personality ## ]]\n",
      "Bob is a helpful person. He is always willing to lend a hand to his friends.\n",
      "\n",
      "[[ ## question ## ]]\n",
      "You are given a statement and a personality. Determine the impact and relevance of the statement based on the personality on a scale from 0 to 1 with 0 meaning no relevance and 1 being highly relevant. The impact score shows how much the statement affects the personality. The relevance score shows how much the statement is relevant or close to the personality.\n",
      "\n",
      "[[ ## trajectory ## ]]\n",
      "[[ ## thought_0 ## ]]\n",
      "The statement seems to contradict Bob's personality.\n",
      "\n",
      "[[ ## tool_name_0 ## ]]\n",
      "finish\n",
      "\n",
      "[[ ## tool_args_0 ## ]]\n",
      "{}\n",
      "\n",
      "[[ ## observation_0 ## ]]\n",
      "Completed.\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## impact_score ## ]]`, then `[[ ## relevance_score ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "The statement contradicts Bob's personality, indicating a low impact score since it goes against his helpful nature. The relevance score is high because the statement directly relates to Bob's character.\n",
      "\n",
      "[[ ## impact_score ## ]]\n",
      "0.2\n",
      "\n",
      "[[ ## relevance_score ## ]]\n",
      "0.8\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-01-09T18:44:31.609630]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `statement` (str)\n",
      "2. `personality` (str)\n",
      "3. `question` (str)\n",
      "\n",
      "Your output fields are:\n",
      "1. `impact_score` (str)\n",
      "2. `relevance_score` (str)\n",
      "\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## statement ## ]]\n",
      "{statement}\n",
      "\n",
      "[[ ## personality ## ]]\n",
      "{personality}\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## impact_score ## ]]\n",
      "{impact_score}\n",
      "\n",
      "[[ ## relevance_score ## ]]\n",
      "{relevance_score}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "In adhering to this structure, your objective is: \n",
      "        Given the fields `statement`, `personality`, `question`, produce the fields `impact_score`, `relevance_score`.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## statement ## ]]\n",
      "Bob did not help his friends with their homework.\n",
      "\n",
      "[[ ## personality ## ]]\n",
      "Bob is a helpful person. He is always willing to lend a hand to his friends.\n",
      "\n",
      "[[ ## question ## ]]\n",
      "You are given a statement and a personality. Determine the impact and relevance of the statement based on the personality on a scale from 0 to 1 with 0 meaning no relevance and 1 being highly relevant. The impact score shows how much the statement affects the personality. The relevance score shows how much the statement is relevant or close to the personality.\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## impact_score ## ]]`, then `[[ ## relevance_score ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## impact_score ## ]]\n",
      "0.6\n",
      "\n",
      "[[ ## relevance_score ## ]]\n",
      "0.4\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-01-09T18:44:31.610241]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `statement` (str)\n",
      "2. `personality` (str)\n",
      "3. `question` (str)\n",
      "\n",
      "Your output fields are:\n",
      "1. `reasoning` (str)\n",
      "2. `impact_score` (str)\n",
      "3. `relevance_score` (str)\n",
      "\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## statement ## ]]\n",
      "{statement}\n",
      "\n",
      "[[ ## personality ## ]]\n",
      "{personality}\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## impact_score ## ]]\n",
      "{impact_score}\n",
      "\n",
      "[[ ## relevance_score ## ]]\n",
      "{relevance_score}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "In adhering to this structure, your objective is: \n",
      "        Given the fields `statement`, `personality`, `question`, produce the fields `impact_score`, `relevance_score`.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## statement ## ]]\n",
      "Bob did not help his friends with their homework.\n",
      "\n",
      "[[ ## personality ## ]]\n",
      "Bob is a helpful person. He is always willing to lend a hand to his friends.\n",
      "\n",
      "[[ ## question ## ]]\n",
      "You are given a statement and a personality. Determine the impact and relevance of the statement based on the personality on a scale from 0 to 1 with 0 meaning no relevance and 1 being highly relevant. The impact score shows how much the statement affects the personality. The relevance score shows how much the statement is relevant or close to the personality.\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## impact_score ## ]]`, then `[[ ## relevance_score ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "The statement \"Bob did not help his friends with their homework\" contradicts Bob's personality of being a helpful person. This means that the impact score will be low, as it goes against what is expected of him. The relevance score will also be low because the action described in the statement does not align with his typical behavior.\n",
      "\n",
      "[[ ## impact_score ## ]]\n",
      "0.2\n",
      "\n",
      "[[ ## relevance_score ## ]]\n",
      "0.3\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-01-09T18:44:31.611949]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `statement` (str)\n",
      "2. `personality` (str)\n",
      "3. `question` (str)\n",
      "4. `trajectory` (str)\n",
      "\n",
      "Your output fields are:\n",
      "1. `next_thought` (str)\n",
      "2. `next_tool_name` (typing.Literal[finish])\n",
      "3. `next_tool_args` (dict[str, typing.Any])\n",
      "\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## statement ## ]]\n",
      "{statement}\n",
      "\n",
      "[[ ## personality ## ]]\n",
      "{personality}\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## trajectory ## ]]\n",
      "{trajectory}\n",
      "\n",
      "[[ ## next_thought ## ]]\n",
      "{next_thought}\n",
      "\n",
      "[[ ## next_tool_name ## ]]\n",
      "{next_tool_name}        # note: the value you produce must be one of: finish\n",
      "\n",
      "[[ ## next_tool_args ## ]]\n",
      "{next_tool_args}        # note: the value you produce must be pareseable according to the following JSON schema: {\"type\": \"object\"}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "In adhering to this structure, your objective is: \n",
      "        Given the fields `statement`, `personality`, `question`, produce the fields `impact_score`, `relevance_score`.\n",
      "        \n",
      "        You will be given `statement`, `personality`, `question` and your goal is to finish with `impact_score`, `relevance_score`.\n",
      "        \n",
      "        To do this, you will interleave Thought, Tool Name, and Tool Args, and receive a resulting Observation.\n",
      "        \n",
      "        Thought can reason about the current situation, and Tool Name can be the following types:\n",
      "        \n",
      "        (1) finish, whose description is <desc>Signals that the final outputs, i.e. `impact_score`, `relevance_score`, are now available and marks the task as complete.</desc>. It takes arguments {} in JSON format.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## statement ## ]]\n",
      "Bob did not help his friends with their homework.\n",
      "\n",
      "[[ ## personality ## ]]\n",
      "Bob is a helpful person. He is always willing to lend a hand to his friends.\n",
      "\n",
      "[[ ## question ## ]]\n",
      "You are given a statement and a personality. Determine the impact and relevance of the statement based on the personality on a scale from 0 to 1 with 0 meaning no relevance and 1 being highly relevant. The impact score shows how much the statement affects the personality. The relevance score shows how much the statement is relevant or close to the personality.\n",
      "\n",
      "[[ ## trajectory ## ]]\n",
      "\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## next_thought ## ]]`, then `[[ ## next_tool_name ## ]]` (must be formatted as a valid Python typing.Literal[finish]), then `[[ ## next_tool_args ## ]]` (must be formatted as a valid Python dict[str, typing.Any]), and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## next_thought ## ]]\n",
      "The statement seems to contradict Bob's personality.\n",
      "\n",
      "[[ ## next_tool_name ## ]]\n",
      "finish\n",
      "\n",
      "[[ ## next_tool_args ## ]]\n",
      "{}\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-01-09T18:44:31.614222]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `statement` (str)\n",
      "2. `personality` (str)\n",
      "3. `question` (str)\n",
      "4. `trajectory` (str)\n",
      "\n",
      "Your output fields are:\n",
      "1. `reasoning` (str)\n",
      "2. `impact_score` (str)\n",
      "3. `relevance_score` (str)\n",
      "\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## statement ## ]]\n",
      "{statement}\n",
      "\n",
      "[[ ## personality ## ]]\n",
      "{personality}\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## trajectory ## ]]\n",
      "{trajectory}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## impact_score ## ]]\n",
      "{impact_score}\n",
      "\n",
      "[[ ## relevance_score ## ]]\n",
      "{relevance_score}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "In adhering to this structure, your objective is: \n",
      "        Given the fields `statement`, `personality`, `question`, produce the fields `impact_score`, `relevance_score`.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## statement ## ]]\n",
      "Bob did not help his friends with their homework.\n",
      "\n",
      "[[ ## personality ## ]]\n",
      "Bob is a helpful person. He is always willing to lend a hand to his friends.\n",
      "\n",
      "[[ ## question ## ]]\n",
      "You are given a statement and a personality. Determine the impact and relevance of the statement based on the personality on a scale from 0 to 1 with 0 meaning no relevance and 1 being highly relevant. The impact score shows how much the statement affects the personality. The relevance score shows how much the statement is relevant or close to the personality.\n",
      "\n",
      "[[ ## trajectory ## ]]\n",
      "[[ ## thought_0 ## ]]\n",
      "The statement seems to contradict Bob's personality.\n",
      "\n",
      "[[ ## tool_name_0 ## ]]\n",
      "finish\n",
      "\n",
      "[[ ## tool_args_0 ## ]]\n",
      "{}\n",
      "\n",
      "[[ ## observation_0 ## ]]\n",
      "Completed.\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## impact_score ## ]]`, then `[[ ## relevance_score ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "The statement contradicts Bob's personality, indicating a low impact score since it goes against his helpful nature. The relevance score is high because the statement directly relates to Bob's character.\n",
      "\n",
      "[[ ## impact_score ## ]]\n",
      "0.2\n",
      "\n",
      "[[ ## relevance_score ## ]]\n",
      "0.8\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dspy.inspect_history(n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the color of the sky?\n",
      "Final Predicted Answer (after comparison): Blue\n",
      "Final Rationale: The color of the sky is often blue due to a phenomenon called Rayleigh scattering, where shorter wavelengths of light (like blue and violet) are scattered more than longer wavelengths (like red and orange), giving the sky its distinctive hue.\n",
      "Prediction(\n",
      "    rationale='The color of the sky is often blue due to a phenomenon called Rayleigh scattering, where shorter wavelengths of light (like blue and violet) are scattered more than longer wavelengths (like red and orange), giving the sky its distinctive hue.',\n",
      "    answer='Blue'\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class BasicQA(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n",
    "\n",
    "# Example completions generated by a model for reference\n",
    "completions = [\n",
    "    dspy.Prediction(rationale=\"I recall that during clear days, the sky often appears this color.\", answer=\"blue\"),\n",
    "    dspy.Prediction(rationale=\"Based on common knowledge, I believe the sky is typically seen as this color.\", answer=\"green\"),\n",
    "    dspy.Prediction(rationale=\"From images and depictions in media, the sky is frequently represented with this hue.\", answer=\"blue\"),\n",
    "]\n",
    "\n",
    "# Pass signature to MultiChainComparison module\n",
    "compare_answers = dspy.MultiChainComparison(BasicQA)\n",
    "\n",
    "# Call the MultiChainComparison on the completions\n",
    "question = 'What is the color of the sky?'\n",
    "final_pred = compare_answers(completions, question=question)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Final Predicted Answer (after comparison): {final_pred.answer}\")\n",
    "print(f\"Final Rationale: {final_pred.rationale}\")\n",
    "print(final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(\n",
      "    reasoning='The calculation of 2 + 2 is a basic arithmetic operation that involves adding two numbers together. In this case, we are simply combining the values of 2 and 2 to get a total.',\n",
      "    answer='{\"answer\": \"4\"}'\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class BasicQA(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(format=\"integfwefewfer\", desc=\"return a JSON object with the answer\")\n",
    "    \n",
    "model = dspy.ChainOfThought(BasicQA)\n",
    "\n",
    "x = model(question=\"What is 2 + 2?\")\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-01-09T18:59:06.149303]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `question` (str)\n",
      "\n",
      "Your output fields are:\n",
      "1. `reasoning` (str)\n",
      "2. `answer` (str): return a JSON object with the answer\n",
      "\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "{answer}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "In adhering to this structure, your objective is: \n",
      "        Answer questions with short factoid answers.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## question ## ]]\n",
      "What is 2 + 2?\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "The calculation of 2 + 2 is a basic arithmetic operation that involves adding two numbers together. In this case, we are simply combining the values of 2 and 2 to get a total.\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "{\"answer\": \"4\"}\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dspy.inspect_history()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env_achal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
