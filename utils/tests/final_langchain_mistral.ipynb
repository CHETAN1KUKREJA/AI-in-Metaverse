{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def kill_gpu_processes():\n",
    "    try:\n",
    "        # Get nvidia-smi output\n",
    "        output = subprocess.check_output(['nvidia-smi', '--query-compute-apps=pid', '--format=csv,noheader']).decode()\n",
    "        \n",
    "        # Get list of PIDs\n",
    "        pids = [int(pid) for pid in output.strip().split('\\n') if pid]\n",
    "        \n",
    "        # Kill each process\n",
    "        for pid in pids:\n",
    "            try:\n",
    "                os.kill(pid, 9)\n",
    "                print(f\"Killed process {pid}\")\n",
    "            except ProcessLookupError:\n",
    "                continue\n",
    "            \n",
    "        return len(pids)\n",
    "    except FileNotFoundError:\n",
    "        print(\"nvidia-smi not found. Is NVIDIA driver installed?\")\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    killed = kill_gpu_processes()\n",
    "    print(f\"Killed {killed} GPU processes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Required Libraries\n",
    "Install the necessary libraries using pip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login to Hugging Face Hub\n",
    "Log in to Hugging Face Hub using the provided token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in to Hugging Face Hub!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace 'your_token_here' with your Hugging Face access token\n",
    "token_sukhvansh = \"\"\n",
    "\n",
    "# Log in to Hugging Face Hub\n",
    "login(token_sukhvansh)\n",
    "\n",
    "print(\"Successfully logged in to Hugging Face Hub!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, including langchain, transformers, and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "import warnings\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers.models.mistral.modeling_mistral import MistralForCausalLM\n",
    "from transformers.models.llama.tokenization_llama_fast import LlamaTokenizerFast\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from typing import Optional, List, Mapping, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Tools\n",
    "Define the tools that will be used in the environment, such as goto, talk, trade, eat, and collect_apples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def goto(location: str, speed: int) -> str:\n",
    "    \"\"\"Go to a specific location in the environment.\n",
    "    \n",
    "    Args:\n",
    "        location: The target location to go to. Must be a valid location in the environment.\n",
    "        speed: The speed at which you want to travel to the location.\n",
    "    \"\"\"\n",
    "    return \"Reached the location.\"\n",
    "\n",
    "@tool\n",
    "def talk(agent_name: str, message: str) -> str:\n",
    "    \"\"\"Talk to another agent.\n",
    "    \n",
    "    Args:\n",
    "        agent_name: Name of the agent you want to communicate to.\n",
    "        message: The message you want to communicate to the other agent.\n",
    "    \"\"\"\n",
    "    return \"Message communicated.\"\n",
    "\n",
    "@tool\n",
    "def trade(agent_name: str, amount_of_money: int, amount_of_apples: int) -> int:\n",
    "    \"\"\"Trade with another agent.\n",
    "    \n",
    "    Args:\n",
    "        agent_name: The name of the agent to trade with.\n",
    "        amount_of_money: Amount of money you are willing to take.\n",
    "        amount_of_apples: Number of apples you are willing to give.\n",
    "    \"\"\"\n",
    "    return 50\n",
    "\n",
    "@tool\n",
    "def eat(number_of_apples: int) -> None:\n",
    "    \"\"\"Eat a specified number of apples.\n",
    "    \n",
    "    Args:\n",
    "        number_of_apples: Number of apples to eat.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "@tool\n",
    "def collect_apples(number_of_apples: int) -> None:\n",
    "    \"\"\"Collect a specified number of apples.\n",
    "    \n",
    "    Args:\n",
    "        number_of_apples: Number of apples to collect.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model and Tokenizer\n",
    "Load the model and tokenizer from Hugging Face using the specified model name and quantization configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load Model and Tokenizer\n",
    "\n",
    "# Define the model name and quantization configuration\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "# Load the model and tokenizer from Hugging Face\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, quantization_config=quantization_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Custom LLM Class\n",
    "Define a custom LLM class that extends the base LLM class and implements the required methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLLMMistral(LLM):\n",
    "    model: MistralForCausalLM\n",
    "    tokenizer: LlamaTokenizerFast\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None,\n",
    "              run_manager: Optional[CallbackManagerForLLMRun] = None) -> str:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "\n",
    "        encodeds = self.tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "        model_inputs = encodeds.to(self.model.device)\n",
    "\n",
    "        generated_ids = self.model.generate(model_inputs, max_new_tokens=512, do_sample=True, pad_token_id=tokenizer.eos_token_id, top_k=4, temperature=0.7)\n",
    "        decoded = self.tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "        output = decoded[0].split(\"[/INST]\")[1].replace(\"</s>\", \"\").strip()\n",
    "\n",
    "        if stop is not None:\n",
    "            for word in stop:\n",
    "                output = output.split(word)[0].strip()\n",
    "\n",
    "        while not output.endswith(\"```\"):\n",
    "            output += \"`\"\n",
    "\n",
    "        return output\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"model\": self.model}\n",
    "\n",
    "llm = CustomLLMMistral(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Prompt Templates\n",
    "Create the system and human prompt templates using ChatPromptTemplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "system=\"\"\"\n",
    "You are designed to solve tasks. Each task requires multiple steps that are represented by a markdown code snippet of a json blob.\n",
    "The json structure should contain the following keys:\n",
    "thought -> your thoughts\n",
    "action -> name of a tool\n",
    "action_input -> parameters to send to the tool\n",
    "\n",
    "These are the tools you can use: {tool_names}.\n",
    "\n",
    "These are the tools descriptions:\n",
    "\n",
    "{tools}\n",
    "\n",
    "If you have enough information to answer the query use the tool \"Final Answer\". Its parameters is the solution.\n",
    "If there is not enough information, keep trying.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "human=\"\"\"\n",
    "Add the word \"STOP\" after each markdown snippet. Example:\n",
    "\n",
    "```json\n",
    "{{\"thought\": \"<your thoughts>\",\n",
    " \"action\": \"<tool name or Final Answer to give a final answer>\",\n",
    " \"action_input\": \"<tool parameters or the final output\"}}\n",
    "```\n",
    "STOP\n",
    "\n",
    "This is my query=\"{input}\". Write only the next step needed to solve it.\n",
    "Your answer should be based in the previous tools executions, even if you think you know the answer.\n",
    "Remember to add STOP after each snippet.\n",
    "\n",
    "These were the previous steps given to solve this query and the information you already gathered:\n",
    "\"\"\"\n",
    "\n",
    "prompt_example = \"\"\"you are an LLM agent that is supposed to act like a human character in a virtual environment. you are given some set of actions and your job is to choose the most relevant sequence of actions in order to carry out a task in the environment. In this environment there is a forest to collect to apples with a limited supply per day and you use money to trade apples. There is a trade centre where trades can occur with other agents, and there is a house where agents can sleep.\n",
    "\n",
    "your character description: your name is Bob, you have money 50 euros and 20 apples.\n",
    "\n",
    "You are given the environment information as follows: Your location is forest in the metaverse and there is Maria agent in the forest.\n",
    "\n",
    "Current local memory your agent has: [{'action': 'goto', 'action_input': 'forest'}, {'action': 'talk',\n",
    "  'action_input': {'agent_name': 'Maria',\n",
    "   'message': 'Hello Maria, would you be interested in trading apples for money?'}},maria_reply: No I don't have any apples, {'action': 'goto', 'action_input': 'forest'}]\n",
    "\n",
    "Current actions which may have happened which concerns you: None\n",
    "\n",
    "Your goal is always to maximise the amount of money that you have and generate a valid sequence of actions you choose to do at that particular instant of time.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are an LLM agent that is supposed to act like a human character in a virtual environment. you are given some set of actions and your job is to choose the most relevant sequence of actions in order to carry out a task in the environment.\n",
      "\n",
      "### world description\n",
      "The current time in your world is 15.12.2024 10am, There are a number of locations in your world and their descriptions are as follows:\n",
      "The location market is one which can be used to Trade objects with other agents. But it's closed now, currently there is no agents there. Reopen time is unknown. and is currently at a distance of 2.8284271247461903 from you and you are can_see it. The location forest is one which can be used to You can pick fruit in the forest. The fruit can be used trade. and is currently at a distance of 25.45584412271571 from you and you are can_see it. The location outside is one which can be used to go to locations and is currently at a distance of 11.313708498984761 from you and you are inside it.\n",
      "\n",
      "### vicinity description\n",
      "\n",
      "the following audio which may or may not concern you was said in your vicinity:\n",
      "\n",
      "\n",
      "### agent description\n",
      "\n",
      "Your current status:\n",
      "- Health: 20\n",
      "- Hunger: 3\n",
      "- Happiness: 5\n",
      "- Age: 23\n",
      "- Current Location: outside\n",
      "- Current Action: None (-1% completed)\n",
      "- Items in Hand: money (10)\n",
      "- Inventory: \n",
      "- Ownership: \n",
      "\n",
      "### contracts description\n",
      "You are not engaged in any contracts.\n",
      "\n",
      "### system description\n",
      "\n",
      "\n",
      "### end of description\n",
      "That's all the information you know for now. If you need information from other agents, you should ask them and wait them to reply. You are not allowed to assume anything yourself.\n",
      "\n",
      "### goal\n",
      "Your goal is most of the time to maximise the amount of money that you have.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from prompt.base import get_prompt\n",
    "import json\n",
    "\n",
    "with open(\"test_json.json\", \"r\") as f:\n",
    "    input_json = json.load(f)\n",
    "\n",
    "# prompt = prompt_example\n",
    "prompt = get_prompt(input_json, \"simple_chain\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Agent and Executor\n",
    "Create the agent and executor using the defined tools, LLM, and prompt templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [goto, talk, trade, eat, collect_apples]\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_system = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", human),\n",
    "        MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain.agents import create_json_chat_agent, AgentExecutor\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "agent = create_json_chat_agent(\n",
    "    tools = tools,\n",
    "    llm = llm,\n",
    "    prompt = prompt_system,\n",
    "    stop_sequence = [\"STOP\"],\n",
    "    template_tool_response = \"{observation}\"\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Agent Executor\n",
    "Run the agent executor with a sample prompt to demonstrate the functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_to_end = ['talk', 'trade', 'goto', 'collect_apples']\n",
    "tools_all = ['talk', 'trade', 'goto', 'eat', 'collect_apples']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tools_all are all the tools that the agent can use to interact with the environment and the tools_to_end are all the tools on which the chain terminates as after that interaction with the game engine is required with their feedback. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Json_output_calls = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Json_output_calls are the final set of actions for that particular run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new None chain...\u001b[0m\n",
      "{'actions': [AgentAction(tool='talk', tool_input={'agent_name': 'myself', 'message': 'Should I go to the forest or wait for the market to reopen?'}, log='```json\\n{\"thought\": \"The market is currently closed and there are no agents present, so I cannot trade there. I could go to the forest, where I can pick fruit and possibly trade it, or wait for the market to reopen.\",\\n \"action\": \"talk\",\\n \"action_input\": {\\n    \"agent_name\": \"myself\",\\n    \"message\": \"Should I go to the forest or wait for the market to reopen?\"\\n }\\n}```')], 'messages': [AIMessage(content='```json\\n{\"thought\": \"The market is currently closed and there are no agents present, so I cannot trade there. I could go to the forest, where I can pick fruit and possibly trade it, or wait for the market to reopen.\",\\n \"action\": \"talk\",\\n \"action_input\": {\\n    \"agent_name\": \"myself\",\\n    \"message\": \"Should I go to the forest or wait for the market to reopen?\"\\n }\\n}```', additional_kwargs={}, response_metadata={})]}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for step in agent_executor.stream({\"input\": prompt}):\n",
    "    print(step)\n",
    "    if step['actions'][0].tool not in tools_all:\n",
    "        print(\"hallucination\")\n",
    "        break\n",
    "    curr_step = {}\n",
    "    curr_step['action'] = step['actions'][0].tool\n",
    "    curr_step['action_input'] = step['actions'][0].tool_input\n",
    "    Json_output_calls.append(curr_step)\n",
    "    if step['actions'][0].tool in tools_to_end:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'action': 'goto', 'action_input': {'location': 'forest', 'speed': 1}}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Json_output_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing_llmbackend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
