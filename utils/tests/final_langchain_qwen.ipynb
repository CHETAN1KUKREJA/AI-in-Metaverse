{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def kill_gpu_processes():\n",
    "    try:\n",
    "        # Get nvidia-smi output\n",
    "        output = subprocess.check_output(['nvidia-smi', '--query-compute-apps=pid', '--format=csv,noheader']).decode()\n",
    "        \n",
    "        # Get list of PIDs\n",
    "        pids = [int(pid) for pid in output.strip().split('\\n') if pid]\n",
    "        \n",
    "        # Kill each process\n",
    "        for pid in pids:\n",
    "            try:\n",
    "                os.kill(pid, 9)\n",
    "                print(f\"Killed process {pid}\")\n",
    "            except ProcessLookupError:\n",
    "                continue\n",
    "            \n",
    "        return len(pids)\n",
    "    except FileNotFoundError:\n",
    "        print(\"nvidia-smi not found. Is NVIDIA driver installed?\")\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    killed = kill_gpu_processes()\n",
    "    print(f\"Killed {killed} GPU processes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Required Libraries\n",
    "Install the necessary libraries using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (0.3.11)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain) (3.11.10)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.24 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain) (0.3.25)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain) (0.2.3)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain) (2.10.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: sniffio in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain) (3.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n",
      "Requirement already satisfied: accelerate in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from accelerate) (0.26.5)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n",
      "Requirement already satisfied: requests in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "Requirement already satisfied: bitsandbytes in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (0.45.0)\n",
      "Requirement already satisfied: torch in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from bitsandbytes) (2.5.1)\n",
      "Requirement already satisfied: numpy in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: typing_extensions>=4.8.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: networkx in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "# Install the necessary libraries using pip\n",
    "!pip install langchain\n",
    "!pip install accelerate\n",
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login to Hugging Face Hub\n",
    "Log in to Hugging Face Hub using the provided token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student1/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in to Hugging Face Hub!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace 'your_token_here' with your Hugging Face access token\n",
    "token_sukhvansh = \"\"\n",
    "\n",
    "# Log in to Hugging Face Hub\n",
    "login(token_sukhvansh)\n",
    "\n",
    "print(\"Successfully logged in to Hugging Face Hub!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, including langchain, transformers, and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "import warnings\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers.models.mistral.modeling_mistral import MistralForCausalLM\n",
    "from transformers.models.llama.tokenization_llama_fast import LlamaTokenizerFast\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from typing import Optional, List, Mapping, Any\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Tools\n",
    "Define the tools that will be used in the environment, such as goto, talk, trade, eat, and collect_apples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def goto(location: str, speed: int) -> str:\n",
    "    \"\"\"Go to a specific location in the environment.\n",
    "    \n",
    "    Args:\n",
    "        location: The target location to go to. Must be a valid location in the environment.\n",
    "        speed: The speed at which you want to travel to the location.\n",
    "    \"\"\"\n",
    "    return \"Reached the location.\"\n",
    "\n",
    "@tool\n",
    "def talk(agent_name: str, message: str) -> str:\n",
    "    \"\"\"Talk to another agent.\n",
    "    \n",
    "    Args:\n",
    "        agent_name: Name of the agent you want to communicate to.\n",
    "        message: The message you want to communicate to the other agent.\n",
    "    \"\"\"\n",
    "    return \"Message communicated.\"\n",
    "\n",
    "@tool\n",
    "def trade(agent_name: str, amount_of_money: int, amount_of_apples: int) -> int:\n",
    "    \"\"\"Trade with another agent.\n",
    "    \n",
    "    Args:\n",
    "        agent_name: The name of the agent to trade with.\n",
    "        amount_of_money: Amount of money you are willing to take.\n",
    "        amount_of_apples: Number of apples you are willing to give.\n",
    "    \"\"\"\n",
    "    return 50\n",
    "\n",
    "@tool\n",
    "def eat(number_of_apples: int) -> None:\n",
    "    \"\"\"Eat a specified number of apples.\n",
    "    \n",
    "    Args:\n",
    "        number_of_apples: Number of apples to eat.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "@tool\n",
    "def collect_apples(number_of_apples: int) -> None:\n",
    "    \"\"\"Collect a specified number of apples.\n",
    "    \n",
    "    Args:\n",
    "        number_of_apples: Number of apples to collect.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model and Tokenizer\n",
    "Load the model and tokenizer from Hugging Face using the specified model name and quantization configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m quantization_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load the model and tokenizer from Hugging Face\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "File \u001b[0;32m~/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages/transformers/modeling_utils.py:4207\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4204\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[1;32m   4206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4207\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4209\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4210\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/miniconda3/envs/testing_llmbackend/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:102\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 102\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    103\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         )\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Mapping, Any\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "\n",
    "# First load model and tokenizer\n",
    "model_name = \"unsloth/Qwen2.5-14B-Instruct-bnb-4bit\"\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "# Load the model and tokenizer from Hugging Face\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    quantization_config=quantization_config, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Custom LLM Class\n",
    "Define a custom LLM class that extends the base LLM class and implements the required methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom LLM class\n",
    "class CustomLLMQwen(LLM):\n",
    "    model: Any  # Changed from Qwen2ForCausalLM to Any\n",
    "    tokenizer: Any  # Changed from Qwen2TokenizerFast to Any\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "        \n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None) -> str:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "        \n",
    "        encodeds = self.tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "        model_inputs = encodeds.to(self.model.device)\n",
    "        \n",
    "        generated_ids = self.model.generate(\n",
    "            model_inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            top_k=4,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        decoded = self.tokenizer.batch_decode(generated_ids)\n",
    "        \n",
    "        # Extract the JSON part between ``` and STOP\n",
    "        try:\n",
    "            output = decoded[0]\n",
    "            # Find the last json block\n",
    "            if \"```json\" in output:\n",
    "                json_part = output.split(\"```json\")[-1]\n",
    "                if \"STOP\" in json_part:\n",
    "                    json_part = json_part.split(\"STOP\")[0]\n",
    "                output = json_part.strip()\n",
    "            # Clean up any remaining markers\n",
    "            output = output.replace(\"<|im_start|>\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "            \n",
    "            # Handle stop sequences if provided\n",
    "            if stop is not None:\n",
    "                for word in stop:\n",
    "                    if word in output:\n",
    "                        output = output[:output.index(word)].strip()\n",
    "            print(\"-----------------------------------------------------------------------\")\n",
    "            print(output)\n",
    "            return output\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing output: {e}\")\n",
    "            # Return the cleaned but unprocessed output as fallback\n",
    "            return decoded[0].replace(\"<|im_start|>\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"model\": self.model}\n",
    "llm_qwen = CustomLLMQwen(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Prompt Templates\n",
    "Create the system and human prompt templates using ChatPromptTemplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "system=\"\"\"\n",
    "You are designed to solve tasks. Each task requires multiple steps that are represented by a markdown code snippet of a json blob.\n",
    "The json structure should contain the following keys:\n",
    "thought -> your thoughts\n",
    "action -> name of a tool\n",
    "action_input -> parameters to send to the tool\n",
    "\n",
    "These are the tools you can use: {tool_names}.\n",
    "\n",
    "These are the tools descriptions:\n",
    "\n",
    "{tools}\n",
    "\n",
    "If you have enough information to answer the query use the tool \"Final Answer\". Its parameters is the solution.\n",
    "If there is not enough information, keep trying.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "human=\"\"\"\n",
    "Add the word \"STOP\" after each markdown snippet. Example:\n",
    "\n",
    "```json\n",
    "{{\"thought\": \"<your thoughts>\",\n",
    " \"action\": \"<tool name or Final Answer to give a final answer>\",\n",
    " \"action_input\": \"<tool parameters or the final output\"}}\n",
    "```\n",
    "STOP\n",
    "\n",
    "This is my query=\"{input}\". Write only the next step needed to solve it.\n",
    "Your answer should be based in the previous tools executions, even if you think you know the answer.\n",
    "Remember to add STOP after each snippet.\n",
    "\n",
    "These were the previous steps given to solve this query and the information you already gathered:\n",
    "\"\"\"\n",
    "\n",
    "prompt_example = \"\"\"you are an LLM agent that is supposed to act like a human character in a virtual environment. you are given some set of actions and your job is to choose the most relevant sequence of actions in order to carry out a task in the environment. In this environment there is a forest to collect to apples with a limited supply per day and you use money to trade apples. There is a trade centre where trades can occur with other agents, and there is a house where agents can sleep.\n",
    "\n",
    "your character description: your name is Bob, you have money 50 euros and 20 apples.\n",
    "\n",
    "You are given the environment information as follows: Your location is forest in the metaverse and there is Maria agent in the forest.\n",
    "\n",
    "Current local memory your agent has: [{'action': 'goto', 'action_input': 'forest'}, {'action': 'talk',\n",
    "  'action_input': {'agent_name': 'Maria',\n",
    "   'message': 'Hello Maria, would you be interested in trading apples for money?'}},maria_reply: No I don't have any apples, {'action': 'goto', 'action_input': 'forest'}]\n",
    "\n",
    "Current actions which may have happened which concerns you: None\n",
    "\n",
    "Your goal is always to maximise the amount of money that you have and generate a valid sequence of actions you choose to do at that particular instant of time.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are an LLM agent that is supposed to act like a human character in a virtual environment. you are given some set of actions and your job is to choose the most relevant sequence of actions in order to carry out a task in the environment.\n",
      "\n",
      "### world description\n",
      "The current time in your world is 15.12.2024 10am, There are a number of locations in your world and their descriptions are as follows:\n",
      "The location market is one which can be used to Trade objects with other agents. But it's closed now, currently there is no agents there. Reopen time is unknown. and is currently at a distance of 2.8284271247461903 from you and you are can_see it. The location forest is one which can be used to You can pick fruit in the forest. The fruit can be used trade. and is currently at a distance of 25.45584412271571 from you and you are can_see it. The location outside is one which can be used to go to locations and is currently at a distance of 11.313708498984761 from you and you are inside it.\n",
      "\n",
      "### vicinity description\n",
      "\n",
      "the following audio which may or may not concern you was said in your vicinity:\n",
      "\n",
      "\n",
      "### agent description\n",
      "\n",
      "Your current status:\n",
      "- Health: 20\n",
      "- Hunger: 3\n",
      "- Happiness: 5\n",
      "- Age: 23\n",
      "- Current Location: outside\n",
      "- Current Action: None (-1% completed)\n",
      "- Items in Hand: money (10)\n",
      "- Inventory: \n",
      "- Ownership: \n",
      "\n",
      "### contracts description\n",
      "You are not engaged in any contracts.\n",
      "\n",
      "### system description\n",
      "\n",
      "\n",
      "### end of description\n",
      "That's all the information you know for now. If you need information from other agents, you should ask them and wait them to reply. You are not allowed to assume anything yourself.\n",
      "\n",
      "### goal\n",
      "Your goal is most of the time to maximise the amount of money that you have.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from prompt.base import get_prompt\n",
    "import json\n",
    "\n",
    "with open(\"test_json.json\", \"r\") as f:\n",
    "    input_json = json.load(f)\n",
    "\n",
    "# prompt = prompt_example\n",
    "prompt = get_prompt(input_json, \"simple_chain\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Agent and Executor\n",
    "Create the agent and executor using the defined tools, LLM, and prompt templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [goto, talk, trade, eat, collect_apples]\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_system = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", human),\n",
    "        MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain.agents import create_json_chat_agent, AgentExecutor\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "agent = create_json_chat_agent(\n",
    "    tools=tools,\n",
    "    llm=llm_qwen,\n",
    "    prompt=prompt_system,\n",
    "    stop_sequence=[\"STOP\", \"<|im_end|>\"],  # Match Qwen's format\n",
    "    template_tool_response=\"{observation}\"\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Agent Executor\n",
    "Run the agent executor with a sample prompt to demonstrate the functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_to_end = ['talk', 'trade', 'goto', 'collect_apples']\n",
    "tools_all = ['talk', 'trade', 'goto', 'eat', 'collect_apples']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tools_all are all the tools that the agent can use to interact with the environment and the tools_to_end are all the tools on which the chain terminates as after that interaction with the game engine is required with their feedback. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Json_output_calls = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Json_output_calls are the final set of actions for that particular run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent_executor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[43magent_executor\u001b[49m\u001b[38;5;241m.\u001b[39mstream({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(step)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtool \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tools_all:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agent_executor' is not defined"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for step in agent_executor.stream({\"input\": prompt}):\n",
    "    print(step)\n",
    "    if step['actions'][0].tool not in tools_all:\n",
    "        print(\"hallucination\")\n",
    "        break\n",
    "    end = time.time()\n",
    "    curr_step = {}\n",
    "    curr_step['action'] = step['actions'][0].tool\n",
    "    curr_step['action_input'] = step['actions'][0].tool_input\n",
    "    curr_step['time'] = end - start\n",
    "    Json_output_calls.append(curr_step)\n",
    "    start = time.time() \n",
    "    if step['actions'][0].tool in tools_to_end:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'action': 'goto', 'action_input': {'location': 'forest', 'speed': 5}}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "Json_output_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing_llmbackend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
